Bevezetés – A cég bemutatása és a szakmai gyakorlatom szerepe

Szakmai gyakorlatomat a DataEdge Kft. nevű, Szegeden működő informatikai cégnél végzem, amely elsősorban felhőalapú kasszarendszerek fejlesztésével és karbantartásával foglalkozik. A vállalat fő terméke a Hermes nevű rendszer, amely egy modern, skálázható megoldásként szolgál a kereskedelmi egységek számára. A cég küldetése, hogy leváltsa a régi, nehézkesen működő, lokális rendszereket egy korszerű, központilag irányított, mégis bolt-specifikusan testre szabható infrastruktúrával.

A korábbi rendszerfelállásokban minden egyes bolt rendelkezett saját, fizikai kiszolgálóval, amely a háttérfolyamatokat működtette – ideértve a pénztárgépeket, az eladói és önkiszolgáló rendszereket. Ezek a megoldások azonban számos hiányossággal küzdöttek: nem volt lehetőség valós idejű statisztikák készítésére, nem léteztek részletes leltározási vagy értékesítési kimutatások, és a karbantartás is helyi szinten történt, gyakran manuálisan, nehézkesen.

A Hermes rendszer erre kínál egy strukturált, központosított, mégis rugalmas megoldást. A rendszer alapja egy főtörzs, amely minden fontos entitást – például termékeket, beszállítókat, partnereket – egy helyen kezel. Minden új bolt beüzemelésekor csak a hozzá kapcsolódó, releváns adatokat küldjük le a bolt saját D.A.S. környezetébe. Így lehetőség nyílik arra, hogy a boltok teljesen elszeparáltan, saját adatkészlettel működjenek, miközben a háttérben egy egységes rendszer szolgáltatja az adatokat és a funkcionalitást. Ennek köszönhetően a boltvezetés valós idejű információkhoz jut, automatizált vagy kérésre készülő riportokon keresztül, ami jelentősen megkönnyíti a döntéshozatalt, a készletgazdálkodást és az általános működés tervezését.

2025. január 20-a óta teljes munkaidős alkalmazottként dolgozom a cégnél, hivatalosan törzsadatkezelő pozícióban, de a munkaköröm ennél jóval szerteágazóbb. A törzsadatkezelők és a szoftverfejlesztők közötti kapcsolattartóként tevékenykedem, ahol elsődleges feladatom a belső termékszinkronizációs folyamatok biztosítása és fejlesztése. Ez azt jelenti, hogy gondoskodom róla, hogy az új vagy módosított termékadatok – mint például árak, cikkszámok, megnevezések vagy csomagolási információk – pontosan és időben kerüljenek továbbításra a különböző adatbázisok, rendszerek vagy bolt specifikus környezetek felé.

A napi munkavégzésem során szinte kizárólag Python nyelven dolgozom, mivel ez az eszköz a legalkalmasabb arra, hogy automatizált adatfeldolgozási, riportálási és API-kommunikációs feladatokat végezzek. A riportok elkészítése során gyakran használok olyan népszerű Python könyvtárakat, mint a Pandas (adatmanipuláció), SQLAlchemy (adatbázis-kapcsolat és lekérdezések), valamint az openpyxl vagy xlsxwriter, amelyek lehetővé teszik az Excel formátumú exportálást és formázást.

Emellett napi szinten használok SQL nyelvet, különösen PostgreSQL szintaxissal, mivel az általunk használt adatbázisok ebben a formátumban működnek. Az SQL lekérdezéseket jellemzően Python scriptekből hajtom végre, de gyakran dolgozom közvetlenül DBeaver vagy más adatbáziskezelő felületen is. A riportálási feladatok során több adatbázisból gyűjtök információt, majd azokat a riport követelményeinek megfelelően tisztítom, átalakítom és rendszerezem – akár automatikus napi jelentések formájában, akár egyedi kérések alapján.

Feladataim közé tartozik továbbá API-kommunikáció is, ahol REST alapú metódusokat – például GET, POST, PUT – használok annak érdekében, hogy termékadatokat frissítsek vagy új rekordokat rögzítsek a megfelelő rendszer(ek)ben. Ezekhez általában Swagger dokumentáció alapján dolgozom, és figyelek a megfelelő header-ek, tokenek és adatstruktúrák használatára is.

Összességében munkám során nemcsak programozói feladatokat látok el, hanem folyamatosan biztosítom az adatáramlás pontosságát és megbízhatóságát a szervezet rendszerei között. A napi rutinom során egyszerre van jelen a problémamegoldás, az adatmenedzsment, az üzleti igények technikai megvalósítása, és az ehhez szükséges dokumentálás, kommunikáció és riportálás.


2. Munka közben elsajátított eszközhasználat és fejlesztési folyamatok

A mindennapi munkavégzésem során macOS operációs rendszert használok, amelyhez a fejlesztői környezetemet is igazítanom kellett. A választott eszközöm a Visual Studio Code (VS Code), amely egy modern, letisztult és nagy mértékben testreszabható kódszerkesztő, ideális választás Python fejlesztéshez. A VS Code segítségével strukturáltan és átláthatóan tudom kezelni a projektjeimet, különösen akkor, amikor külön modulokba, fájlokba szedett szkriptekben dolgozom, vagy automatizálási logikát építek fel.

Bár a munkám jelentős része nem közvetlenül a parancssorban zajlik, mégis elengedhetetlenné vált, hogy alaposan megismerkedjek a terminál alapú munkavégzés alapjaival. A Mac gépemen számos beállítást kellett elvégeznem ahhoz, hogy gördülékenyen tudjak dolgozni: shell konfigurációk (.zshrc fájl), alias parancsok létrehozása, valamint különböző kiegészítők (például zsh-syntax-highlighting) telepítése is a tanulási folyamat része volt. Ezek a tapasztalatok nagyban hozzájárultak ahhoz, hogy könnyebben és gyorsabban tudjak navigálni a fájlrendszerben, fájlokat, mappákat kezelni, illetve futtatni a Python szkripteket – különösen hasznos ez, amikor egy új projekt struktúráját állítom fel, vagy épp riportgeneráláshoz készítek ideiglenes fájlokat.

A VS Code integrált terminálját a verziókezelési műveletekhez is előszeretettel használom. A DataEdge-nél több fejlesztési projekt is GitLab tárolókban található, így elengedhetetlen volt számomra, hogy megértsem és gyakorlatban is tudjam használni a verziókezelést. Eleinte Git Bash felületen keresztül végeztem az alapvető műveleteket, mint például a git clone, checkout, pull, commit, merge, majd ezeket később VS Code terminálban is képes lettem elvégezni, gördülékenyebben és gyorsabban. Megtanultam, hogyan hozzak létre saját branch-et, hogyan dokumentáljam a változtatásokat, és hogyan pusholjam vissza őket a megfelelő remote repóba. Ez különösen fontos volt olyan projekteknél, mint a cron alapú riportgenerálás, ahol automatizált szkriptek kerülnek verziókezelés alá.

A napi munkám során gyakran kell REST API-kkal kommunikálnom, amihez a Postman nyújt megbízható és praktikus megoldást. A Postman segítségével tudok autentikációs tokeneket lekérni, például a Swagger alapú API dokumentációval rendelkező rendszerekhez, ahol az egyes endpointokat Bearer token segítségével lehet elérni. Itt jellemzően POST vagy PUT metódusokat használok termékadatok létrehozására, módosítására, vagy épp megrendelés állapotok frissítésére. Emellett kezelek egy mailhook jellegű kapcsolódást is, ami a céges emailrendszert figyeli – például visszajelzést ad, ha az üzenetküldés sikertelen volt, vagy szükség van az email interfész újraindítására. (A webhook általánosságban olyan mechanizmus, amely lehetővé teszi, hogy egy alkalmazás automatikusan értesítést küldjön egy másik rendszernek bizonyos események bekövetkeztekor.)

Az adatbázis-kezeléshez a DBeaver alkalmazást használom, amely az egyik legfontosabb eszközöm lett a fejlesztési feladataim során. A DBeaver egy grafikus SQL kliens, amely lehetővé teszi, hogy egyszerre több adatbázis-szerverhez csatlakozzak, lekérdezéseket írjak, táblákat, nézeteket, függvényeket vizsgáljak. Munkámból fakadóan a haladóbb SQL ismeretek fokozatos elsajátítása elengedhetetlen, hiszen gyakran kell összetett lekérdezéseket, aggregációkat, több táblát érintő kapcsolódásokat (JOIN), al-lekérdezéseket (subquery) írnom, miközben biztosítanom kell az adatok pontosságát és konzisztenciáját. A DBeaver-ben az is különösen előnyös, hogy átláthatóan mutatja a sémák, táblák és view-k közötti kapcsolatokat, ami nagy segítség a különböző környezetek közötti eltérések, hiányosságok felismerésében.

A különböző riportkérésekhez sokszor több adatbázisból kell egyszerre adatot kinyerni, összevetni, és értelmezni. Ennek megfelelően alkalmazkodnom kell a mindenkori adatstruktúrához, tudnom kell, hogy mikor melyik adatbázisban, melyik séma alatt található a releváns információ. Sokszor dolgozom PostgreSQL alapú rendszerekben, így a Postgres szintaxis és sajátosságok ismerete alapvető elvárás, amelynek igyekszem minél inkább megfelelni a mindennapokban. A DBeaver és az SQL használata során egyre több gyakorlati tapasztalatot szereztem az adatok lekérdezéséről, szűréséről, módosításáról, és ez a tudás közvetlenül hasznosul a Pythonos riportgenerálásban is, amely gyakran ezekre az SQL alapokra épül.

Összességében ezek az eszközök – a VS Code, a terminál, a Git, a Postman és a DBeaver – a napi munkavégzésem szerves részét képezik, és nemcsak megkönnyítik, hanem lehetővé is teszik, hogy hatékonyan, átláthatóan és megbízhatóan dolgozzak komplex fejlesztési és adatkezelési feladatokon.

3. Projekt: Hermes Analytics – Analitikai környezet előkészítése és adatfeltöltés új kliensekhez

Munkám első heteiben már lehetőségem nyílt egy olyan projektben részt venni, amely mélyebb betekintést adott a vállalat adatarchitektúrájába és az analitikai adatáramlás működésébe. A projekt célja az volt, hogy a Hermes rendszerünk analitikai rétegét új ügyfelek számára is elérhetővé tegyük, strukturált és előkészített formában. Ez a réteg a Hermes Analytics, amely a különböző boltok saját sémáiból táplálkozik, és az adatok alapján egy Superset nevű kimutatáskezelő rendszer képes üzleti szintű riportokat és vizualizációkat generálni.

A háttérben az történik, hogy minden bolt külön schema alatt szerepel az adatbázisban – ez a D.A.S. környezet sajátossága. A script, amelyet készítettem, teljes egészében automatizálja az új ügyfélre szabott analitikai környezet létrehozását. Ez magában foglalja a schema létrehozását, az analitikai táblák generálását és azok feltöltését a szükséges adatokkal – például a cikkek törzsadataival, készletinformációival és forgalmi statisztikáival.

A program működését tekintve modulárisan van felépítve, ami azt jelenti, hogy külön Python fájlban szerepelnek a SQL lekérdezések (query.py), az adatbázis-konfigurációk (db_config.py), és maga a fő futtatási logika (__main__.py). Ez a szerkezet nemcsak karbantarthatóvá teszi a rendszert, hanem jól skálázható is: ha új típusú adatokat kell betölteni, vagy új endpointok jelennek meg, azokat egyszerűen be lehet illeszteni a meglévő struktúrába. A program alapvetően szekvenciális, procedurális logikát követ, lépésről lépésre haladva végig az adatok lekérdezésétől az adatbázisba való betöltésig. A try-except blokkok, ellenőrzések és naplózás (log) pedig stabilitást és hibakezelést biztosítanak.

A lekérdezések végrehajtása és az adatbetöltés során kulcsszerepet kapott a pandas könyvtár, amelyet a program az adatok kezelésére használ. A pandas lehetővé teszi, hogy a lekérdezett SQL eredményeket DataFrame formájában kezeljem, és azokon különféle szűréseket, transzformációkat vagy validálásokat hajtsak végre, mielőtt azok visszakerülnek az adatbázisba. Mivel a to_sql() metódussal direktben tudom az adatokat PostgreSQL adatbázisba tölteni, ez nagymértékben leegyszerűsíti a műveletet.

Nem véletlen, hogy a pandas használatát már a gyakorlatom legelején középpontba helyeztük. Az volt a cél, hogy közel kerüljek az adatelemzés (Data Analytics) világához, és ehhez a pandas egy kiváló eszköz. Olyan technikákat kellett megtanulnom, mint az adatok tisztítása (dropna, fillna, astype), aggregálása (groupby, sum, count), sor- és oszlopműveletek, valamint az adatok struktúrált exportálása különféle formátumokba (CSV, Excel). Ezek a készségek nemcsak ebben a projektben, hanem hosszútávon is kulcsfontosságúak lesznek, akár riportgenerálás, akár adatellenőrzési folyamatok során.

A jelenlegi megvalósítás TRUNCATE + INSERT INTO alapon működik, vagyis minden futáskor törli a korábbi adattartalmat és friss adatokkal tölti fel a táblákat. Ugyanakkor már elindult egy UPDATE metódus kialakítása is, amely hosszabb távon lehetőséget nyújt az intelligensebb frissítésre – ezzel elkerülhető lesz a teljes újratöltés, és csökken a rendszer terhelése is.

Ez a projekt nemcsak technikai szempontból jelentett kihívást, hanem felelősséget is, hiszen ezek az adatok közvetlenül kerülnek be a Superset kimutatási rendszerébe, amelyet az ügyfelek üzleti döntések meghozatalához használnak. Már a karrierem elején olyan feladatot kaptam, amelynek eredménye látható, mérhető és éles rendszerbe kerül – ez egyszerre volt megtisztelő és motiváló. A projekt során megszerzett tudás azóta is számos más munkafolyamatban hasznosul, legyen szó riportkészítésről, API-k kiszolgálásáról, vagy adatminőség-ellenőrzésről.


4. Projekt: Kassza naplófigyelő és valós idejű hibajelentés Element chatre

A munkám során egyre több olyan projektbe kapcsolódtam be, ahol a stabilitás és a valós idejű reakcióképesség központi szerepet kapott. Ennek egyik legjobb példája a kassza naplófigyelő rendszer, amelyet egy valós üzleti probléma megoldására hoztunk létre. A projekt célja az volt, hogy a boltok kasszarendszereiből érkező naplóbejegyzésekben automatikusan felismerjük a hibás, ismeretlen vonalkódos tételeket, és ezekről azonnali értesítést küldjünk egy erre dedikált Element chat szobába. Így az operatív csapat valós időben értesülhet a problémáról, és akár perceken belül javítani tudja azt, minimalizálva a kiesést vagy vevői kellemetlenséget.

A megvalósított rendszer egy aszinkron Python alapú megoldás, amely folyamatosan figyeli az adatbázisok meghatározott naplózó tábláit. Jelenleg több különböző bolt (pl. T1_DAS, T6_DAS) szerepel a megfigyelt környezetek között, de a script úgy lett megírva, hogy dinamikusan bővíthető legyen új bolt csatlakoztatásával. A megvalósítás alapját egy végtelen while ciklus adja, amely 10 másodpercenként újra és újra lekéri az új bejegyzéseket, és ha hibás sorokat talál, azokat továbbítja az üzenetküldő rendszer felé.

A program technikai szempontból is érdekes: használja az asyncio könyvtárat, ami lehetővé teszi, hogy párhuzamosan több bolt adatbázisát is figyelje anélkül, hogy a teljes folyamat blokkolódna. A sort_entries_to_chat() függvény minden adatbázisra létrehoz egy külön coroutine-t, így a beérkező hibák detektálása és továbbítása kvázi valós időben történik. A create_knaplo_bejegytracker() nevű inicializáló függvény gondoskodik arról, hogy ne küldjük el ugyanazt a bejegyzést többször, ezzel megelőzve a duplikált értesítéseket.

Az egyik legfontosabb kihívás az volt, hogy a rendszer rendkívül megbízhatóan működjön, hiszen itt valós üzletmenetet érintő hibákról van szó. Az értesítések késése vagy kimaradása közvetlen hatással van a bolt működésére, ezért a scriptnek hibakezeléssel, naplózással és újrapróbálkozási logikával is fel kellett vérteznie magát. A hibás bejegyzések felismerése intelligens szűréssel történik, például ha a termék vonalkódja ismeretlenként jelenik meg, vagy hiányzik egy mező, amely az értékesítéshez szükséges lenne.

A projekt különlegessége, hogy összekapcsolja az adatbázis-kezelést, az aszinkron programozást és a valós idejű kommunikációt, mindezt egy robusztus és üzembiztos rendszerben. A megvalósítás során mélyebben megismertem az asyncio működését, a coroutine-ok használatát, valamint azt, hogyan kell egy hosszú ideig futó, monitorozó programot úgy írni, hogy az ne terhelje túl az adatbázist, mégis friss maradjon minden pillanatban.

A rendszer az Element chat integrációját is tartalmazza. A element_notif_main.py modulban található függvények biztosítják, hogy a hibás bejegyzésekből jól formázott, emberi nyelven értelmezhető üzenetek készüljenek. Ezeket a rendszer azonnal továbbítja a megfelelő csapatnak – így egy esetleges ismeretlen cikkszámot már azelőtt javítani tudnak, hogy az eladás meghiúsulna.

Ez a projekt kiváló példája annak, hogy az automatizálás nemcsak a háttérfolyamatokat tudja hatékonyabbá tenni, hanem közvetlen hatása van a bolt mindennapi működésére is. A megvalósítása során szerzett tapasztalatokat – mint az aszinkron lekérdezések kezelése, időzített ciklusok stabil működtetése, duplikációk kiszűrése – később más riportálási vagy monitoring projektekben is sikeresen hasznosítani tudtam.

5. Projekt: Excel → SQL automatikus konvertáló script – tömeges frissítések a relációs adatbázisban

A napi munkavégzésem során gyakran adódik olyan feladat, amelyben egy Excel formátumban kapott frissítési lista alapján kell tömeges adatváltozásokat végrehajtanom az adatbázisban. Ez különösen igaz a Gyökér törzs adatbázisunk esetében, ahol a relációs táblák (például rel_cikk_szallito_partner) karbantartása történik. Eredetileg ezeket a frissítéseket kézzel kellett volna SQL utasításokká alakítani, ami egy több száz soros tábla esetén időigényes és hibalehetőségekkel teli folyamat. Ezt a problémát oldja meg az általam írt automatikus Excel→SQL konvertáló script.

A Pythonban készült script a pandas könyvtár segítségével beolvassa az Excel fájlt, majd soronként generál hozzá egy-egy UPDATE SQL utasítást, amelyeket egy .sql kimeneti fájlba ír ki (output.sql). A script külön figyelmet fordít arra is, hogy a szállítói cikkszám mező helyesen legyen kezelve: ha a mező üres, akkor NULL értéket generál, míg egyébként pontosan a megadott értéket helyettesíti be. A típuskonverzióhoz használt clean_float_as_str() függvény például segít abban, hogy az Excel fájlban számszerűen tárolt szöveges azonosítók (pl. 12345.0) is helyesen kerüljenek SQL formátumba ('12345'), ne 12345.0 vagy nan alakban.

Ez a script tehát nemcsak időt takarít meg, hanem biztonságosabbá és precízebbé is teszi az adatfrissítéseket. Mivel gyakran kapok olyan táblákat, amelyeket partnercégek vagy kollégák készítenek elő Excelben, fontos volt számomra, hogy olyan eszközt fejlesszek, ami rugalmasan tud alkalmazkodni ezekhez az adatformátumokhoz, és automatikusan képes SQL utasításokká alakítani őket.

A programot modulárisan bővíthető formában írtam meg, így akár más típusú táblák (pl. partneradatok, árak, logisztikai kapcsolatok) frissítésére is fel lehet készíteni. Már most is szerepel benne külön függvény a rel_cikk_szallito_partner frissítésére, de ez könnyedén kiegészíthető más logikákkal is.

A projekt technikailag ugyan nem bonyolult, mégis a gyakorlatban rendkívül hatékony, hiszen:

Lehetővé teszi, hogy ne kelljen több száz sort kézzel SQL-be átírni,

Minimalizálja az elgépelésből vagy formázásból eredő hibákat,

Könnyen futtatható bármikor, ha új .xlsx fájlt kapok.

Ez a megoldás jól példázza, hogy a programozás egyik legnagyobb ereje a hétköznapi feladatok automatizálásában rejlik – legyen az bármilyen "egyszerű" is. A fejlesztés során megszilárdítottam tudásomat a pandas fájlkezelési és adatkonverziós eszközeiben, a fájlműveletekben, és általánosságban abban, hogyan lehet egy gyakorlati probléma köré egy jól működő, újrafelhasználható Python megoldást építeni.